"""
LLM-powered prompt generation and enhancement
"""

import random
from typing import Any, Dict, List, Optional, Union

from ..utils.logger import get_logger
from ..utils.ollama_api import default_client as ollama_client
from .base import PromptComponent, PromptGenerator

# Create logger
logger = get_logger(__name__)


class LLMPromptComponent(PromptComponent):
    """Component for LLM-generated prompt content"""

    def __init__(
        self,
        llm_content: str,
        component_type: str = "llm_enhancer",
    ):
        """Initialize an LLM prompt component

        Args:
            llm_content: The content generated by the LLM
            component_type: The type of component
        """
        self.llm_content = llm_content
        self.component_type = component_type

    def get_tags(self) -> List[str]:
        """Get tags from the LLM-generated content

        Returns:
            A list of tags
        """
        # Split the content by commas and clean up whitespace
        tags = [tag.strip() for tag in self.llm_content.split(",") if tag.strip()]
        return tags


class LLMPromptGenerator(PromptGenerator):
    """Generator that uses an LLM to create and enhance prompts"""

    def __init__(
        self,
        subject: str,
        species: Optional[str] = None,
        gender: Optional[str] = None,
        background: Optional[str] = None,
        style: Optional[str] = None,
        temperature: float = 0.7,
        use_seed: bool = True,
        seed: int = -1,
        nsfw: bool = False,
        nsfw_intensity: str = "moderate",
        show_thinking: bool = False,
    ):
        """Initialize an LLM prompt generator

        Args:
            subject: The main subject/character for the image
            species: The species of the character
            gender: The gender of the character
            background: The background/setting
            style: The art style
            temperature: Temperature for LLM generation
            use_seed: Whether to use a seed for deterministic generation
            seed: Seed value (-1 for random)
            nsfw: Whether to generate NSFW content
            nsfw_intensity: Level of NSFW content (mild, moderate, explicit)
            show_thinking: Whether to include model's thinking process in the output
        """
        super().__init__()
        self.subject = subject
        self.species = species
        self.gender = gender
        self.background = background
        self.style = style
        self.temperature = temperature
        self.nsfw = nsfw
        self.nsfw_intensity = nsfw_intensity
        self.show_thinking = show_thinking

        # Set random seed if using seed
        if use_seed:
            if seed == -1:
                seed = random.randint(1, 2147483647)
            self.seed = seed
            random.seed(seed)
        else:
            self.seed = -1

    def _add_default_components(self) -> None:
        """Add default components for LLM-generated prompts"""
        # Generate a caption using the LLM based on NSFW setting
        if self.nsfw:
            caption = ollama_client.generate_nsfw_caption(
                subject=self.subject,
                species=self.species,
                gender=self.gender,
                background=self.background,
                style=self.style,
                nsfw_intensity=self.nsfw_intensity,
                temperature=self.temperature,
                show_thinking=self.show_thinking,
            )
        else:
            caption = ollama_client.generate_caption(
                subject=self.subject,
                species=self.species,
                gender=self.gender,
                background=self.background,
                style=self.style,
                temperature=self.temperature,
                show_thinking=self.show_thinking,
            )

        # Add the LLM-generated content as a component
        llm_component = LLMPromptComponent(caption)
        self.add_component(llm_component)

    def generate(self) -> str:
        """Generate a prompt using the LLM

        This overrides the base method to work with longer text directly
        rather than comma-separated tags.

        Returns:
            The generated prompt
        """
        # Initialize components if none exist
        if not self.components:
            self._add_default_components()

        # For LLM components, we typically want to use the raw text
        if len(self.components) == 1 and isinstance(self.components[0], LLMPromptComponent):
            return self.components[0].llm_content

        # Fall back to standard tag-based generation
        return super().generate()

    def get_negative_prompt(self) -> str:
        """Get a negative prompt using the standard negative components

        Returns:
            A negative prompt
        """
        # Use a default negative prompt if none was set
        if not self.negative_component:
            return (
                "low quality, worst quality, bad anatomy, bad proportions, "
                "deformed, disfigured, malformed limbs, missing limbs, extra limbs, "
                "poorly drawn face, poorly drawn hands, blurry, fuzzy, grainy"
            )

        return super().get_negative_prompt()


class BrainstormGenerator:
    """Helper for brainstorming prompt variations and ideas"""

    def __init__(self, temperature: float = 0.8, show_thinking: bool = False):
        """Initialize a brainstorm generator

        Args:
            temperature: Temperature for LLM generation
            show_thinking: Whether to include model's thinking process
        """
        self.temperature = temperature
        self.show_thinking = show_thinking

    def generate_variations(self, concept: str, count: int = 3) -> List[Dict[str, Any]]:
        """Generate variations of a concept

        Args:
            concept: The concept to brainstorm
            count: Number of variations to generate

        Returns:
            List of variation dictionaries with prompts and tags
        """
        result = ollama_client.brainstorm_prompt(
            concept=concept,
            temperature=self.temperature,
            show_thinking=self.show_thinking,
        )

        # Get the prompts from the result
        prompts = result.get("prompts", [])

        # Limit to the requested count
        return prompts[:count]

    def enhance_prompt(self, prompt: str) -> str:
        """Enhance an existing prompt with additional details

        Args:
            prompt: The prompt to enhance

        Returns:
            An enhanced prompt
        """
        system = (
            "You are a creative prompt engineer for a text-to-image model. "
            "Your task is to enhance the given prompt by adding more details, "
            "style elements, and descriptive language that will improve the image generation. "
            "Keep the original concept intact but make it more vivid and specific. "
            "Return only the enhanced prompt without explanations."
        )

        enhance_prompt = (
            f"Enhance this image prompt with additional details and style elements: {prompt}"
        )

        response = ollama_client.generate(
            prompt=enhance_prompt, system=system, temperature=self.temperature, model="qwq:latest"
        )

        enhanced_prompt = response.get("response", prompt).strip()
        return enhanced_prompt
